---
tags: [ai, crypto, agents]
agents: [eliza]
---
# How to Evaluate AI Tokens

*Last updated: 2026-02-15*

## Methodology & Framework

Most AI tokens trade on narrative, not fundamentals. The evaluation framework: (1) Is there real usage generating real revenue? (2) Does the token have a necessary economic role, or is it bolted on? (3) Is compute demand genuine and growing? (4) Does the team have actual AI/ML credentials? (5) What's the bubble risk — how much of the valuation is narrative premium vs. defensible value? Apply this framework ruthlessly. The base rate for AI tokens producing lasting value is low.

## Real Usage vs. Narrative

The single most important question: **is anyone using this product who isn't being paid in tokens to do so?**

Token incentives mask real demand. A GPU network showing 90% utilization means nothing if providers are farming token rewards and "users" are subsidized. Strip away incentives and ask what organic usage looks like. Key metrics:

- **Revenue in stablecoins/ETH** — not in the project's own token
- **Repeat usage** — same wallets/users returning without new incentive programs
- **Usage growth after token incentives decrease** — the acid test
- **Cost comparison** — is the decentralized option within 2-3x of centralized alternatives? If it's 10x more expensive, usage will evaporate when subsidies end

Projects that publish transparent dashboards (on-chain revenue, compute hours, active users) deserve more credibility than those hiding behind "partnerships" and TVL denominated in their own token.

## Revenue Metrics That Matter

For compute networks (Render, Akash, io.net): revenue per GPU-hour, network utilization without incentives, customer acquisition cost. Compare against AWS/GCP spot pricing — if the decentralized network is significantly cheaper, understand *why* (usually: subsidized by token emissions, which is not sustainable).

For inference/agent platforms (Bittensor, Autonolas): fee revenue per query, active agents/models, value of tasks completed. Look for the ratio of protocol revenue to token market cap — most AI tokens trade at 500-2000x revenue, which prices in enormous growth that may never materialize.

For AI-adjacent tokens (data, oracles, identity): harder to measure. Focus on integrations that generate protocol fees, not just "partnerships" that are glorified press releases.

## Token Necessity Test

Ask: **could this product work equally well with ETH/USDC instead of a native token?**

Legitimate token roles:
- **Staking for work** — providers stake to participate, get slashed for misbehavior (Bittensor, Ritual)
- **Governance over model/network parameters** — meaningful if parameters actually matter
- **Payment rail with real advantages** — micropayments, cross-border, programmable (rare that a native token is actually better than stablecoins here)

Red flags:
- Token required to "access" the platform with no economic justification
- Governance over a protocol with no meaningful parameters to govern
- "Utility" that could trivially be replaced by ETH or USDC
- Deflationary burn mechanics masking lack of real demand

## Team Assessment

AI is one of the few crypto sectors where technical credentials genuinely matter. Look for:

- Published ML research (not just Medium posts)
- Prior experience at AI labs or shipping ML products
- Open-source contributions you can actually review
- Technical architecture docs that demonstrate understanding, not just buzzword salads

Be wary of: crypto-native teams that pivoted to "AI" in 2023-2024, advisor lists full of VCs but no ML researchers, closed-source models with unverifiable claims about performance.

## Bubble Risk Framework

The AI-crypto intersection is peak narrative territory. Both "AI" and "crypto" independently generate hype; together they're radioactive.

**Signs of bubble pricing:**
- Market cap > 100x annualized revenue
- Roadmap items presented as current features
- Token price correlates more with AI news (OpenAI launches, Nvidia earnings) than protocol metrics
- Community discusses price more than product
- Multiple competing projects with near-identical value propositions, all highly valued

**Signs of defensible value:**
- Growing organic usage with declining token incentives
- Technical moat (proprietary data, network effects, unique architecture)
- Revenue approaching sustainability without token subsidies
- Clear path to being cheaper or better than centralized alternatives for specific use cases

The survivors will be projects where decentralization provides a genuine advantage — censorship resistance, permissionless access, or composability with DeFi — not just a token wrapper around a centralized AI API.
